{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'antoni': [3, {1: [0], 2: [0], 6: [0]}], 'brutu': [3, {1: [1], 2: [1], 4: [0]}], 'caeser': [5, {1: [2], 2: [2], 4: [1], 5: [0], 6: [1]}], 'cleopatra': [1, {1: [3]}], 'merci': [5, {1: [4], 3: [0], 4: [2], 5: [1], 6: [2]}], 'worser': [4, {1: [5], 3: [1], 4: [3], 5: [2]}], 'calpurnia': [1, {2: [3]}], 'angel': [3, {7: [0], 8: [0], 9: [0]}], 'fool': [4, {7: [1], 8: [1], 9: [1], 10: [0]}], 'fear': [3, {7: [2], 8: [2], 10: [1]}], 'in': [4, {7: [3], 8: [3], 9: [2], 10: [2]}], 'rush': [4, {7: [4], 8: [4], 9: [3], 10: [3]}], 'to': [4, {7: [5], 8: [5], 9: [4], 10: [4]}], 'tread': [4, {7: [6], 8: [6], 9: [5], 10: [5]}], 'where': [4, {7: [7], 8: [7], 9: [6], 10: [6]}]}\n",
      "7\n",
      "8\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'antoni': [3, {1: [0], 2: [0], 6: [0]}],\n",
       " 'brutu': [3, {1: [1], 2: [1], 4: [0]}],\n",
       " 'caeser': [5, {1: [2], 2: [2], 4: [1], 5: [0], 6: [1]}],\n",
       " 'cleopatra': [1, {1: [3]}],\n",
       " 'merci': [5, {1: [4], 3: [0], 4: [2], 5: [1], 6: [2]}],\n",
       " 'worser': [4, {1: [5], 3: [1], 4: [3], 5: [2]}],\n",
       " 'calpurnia': [1, {2: [3]}],\n",
       " 'angel': [3, {7: [0], 8: [0], 9: [0]}],\n",
       " 'fool': [4, {7: [1], 8: [1], 9: [1], 10: [0]}],\n",
       " 'fear': [3, {7: [2], 8: [2], 10: [1]}],\n",
       " 'in': [4, {7: [3], 8: [3], 9: [2], 10: [2]}],\n",
       " 'rush': [4, {7: [4], 8: [4], 9: [3], 10: [3]}],\n",
       " 'to': [4, {7: [5], 8: [5], 9: [4], 10: [4]}],\n",
       " 'tread': [4, {7: [6], 8: [6], 9: [5], 10: [5]}],\n",
       " 'where': [4, {7: [7], 8: [7], 9: [6], 10: [6]}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "import nltk\n",
    "import math\n",
    "import pandas as pd\n",
    "import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\"\"\" nltk.download('puntk')\n",
    "nltk.download('stopwords') \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from natsort import natsorted\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('to')\n",
    "stop_words.remove('in')\n",
    "stop_words.remove('where')\n",
    "document_of_terms = []\n",
    "document_with_stemming = []\n",
    "document_id = 1\n",
    "positional_index = {}\n",
    "ps = PorterStemmer()\n",
    "file_name = natsorted(os.listdir('../DocumentCollection')) #to sort files in folder\n",
    "for file in file_name :\n",
    "    #1.1# Read 10 Documents\n",
    "    with open(f'../DocumentCollection/{file}', 'r') as f:\n",
    "        document = f.read()\n",
    "    #1.2# apply tokenizing\n",
    "    tokenizing = word_tokenize(document)\n",
    "    for word in tokenizing : \n",
    "        if word not in stop_words :\n",
    "            #1.3# apply stemming\n",
    "            stemming = ps.stem(word)\n",
    "        document_with_stemming.append(stemming)\n",
    "    document_of_terms.append(document_with_stemming)\n",
    "    document_with_stemming = []\n",
    "#print(document_of_terms)\n",
    "#{term : [freq, {docid : []}]}\n",
    "#{antony : [10, {docid : [1,2,3]}]}\n",
    "#2.1# positional index\n",
    "for document in document_of_terms:\n",
    "    for position, term in enumerate(document):\n",
    "        if term in positional_index:\n",
    "            positional_index[term][0] = positional_index[term][0] + 1\n",
    "            if document_id in positional_index[term][1]:\n",
    "                positional_index[term][1][document_id].append(position)\n",
    "            else:\n",
    "                positional_index[term][1][document_id] = [position]\n",
    "        else:\n",
    "            positional_index[term] = []\n",
    "            positional_index[term].append(1)\n",
    "            positional_index[term].append({})\n",
    "            positional_index[term][1][document_id] = [position]\n",
    "    document_id = document_id + 1\n",
    "print(positional_index)\n",
    "#2.5# phrase query\n",
    "outer_array = [[] for i in range(10)]\n",
    "query = \"fools fear\"\n",
    "for wordd in query.split():\n",
    "    word_after_stem = ps.stem(wordd)\n",
    "    if word_after_stem in positional_index.keys():  \n",
    "      for key in positional_index[word_after_stem][1].keys():\n",
    "        if outer_array[key-1] != []:\n",
    "            if outer_array[key-1][-1] == positional_index[word_after_stem][1][key][0] - 1:\n",
    "                    outer_array[key-1].append(positional_index[word_after_stem][1][key][0])\n",
    "        else:\n",
    "                outer_array[key-1].append(positional_index[word_after_stem][1][key][0])\n",
    "for position, arr in enumerate(outer_array, start=1):\n",
    "       if len(arr) == len(query.split()):\n",
    "            print(position)\n",
    "#2.2# tf and weight tf\n",
    "#tf\n",
    "all_terms = []\n",
    "for doc in document_of_terms:\n",
    "    for word in doc:\n",
    "        all_terms.append(word)\n",
    "def term_frequency(doc):\n",
    "    #put each term in dictionary form with value 0\n",
    "    word_count = dict.fromkeys(all_terms, 0) \n",
    "    for word in doc:\n",
    "        #increment the value of each term in dictionary\n",
    "        word_count[word] += 1\n",
    "        # return dictionary of each document\n",
    "    return word_count\n",
    "# convert dictionay form into columns form\n",
    "tf = pd.DataFrame(term_frequency(document_of_terms[0]).values(), index = term_frequency(document_of_terms[0]). keys())\n",
    "for i in range(1, len(document_of_terms)):\n",
    "    tf[i] = term_frequency(document_of_terms[i]).values()\n",
    "#change name of column\n",
    "tf.columns = ['doc' + str(i) for i in range(1, 11)] \n",
    "#w_tf weighted term frequency 1 + log10(tf)\n",
    "def w_tf(x):\n",
    "    if x > 0:\n",
    "        return math.log10(x) + 1\n",
    "    else:\n",
    "        return 0\n",
    "for i in range(1, len(document_of_terms) + 1):\n",
    "    #replace each raw with w_tf\n",
    "    tf['doc'+ str(i)] =  tf['doc'+str(i)].apply(w_tf) \n",
    "#print(tf)\n",
    "#2.3# IDF\n",
    "df_and_IDF = pd.DataFrame(columns=('d_f','idf'))\n",
    "for i in range(len(tf)):\n",
    "    doc_freq = tf.iloc[i].values.sum()\n",
    "    df_and_IDF.loc[i, 'd_f'] = doc_freq\n",
    "    df_and_IDF.loc[i, 'idf'] = math.log10(10/float(doc_freq))\n",
    "df_and_IDF.index = tf.index\n",
    "#print(df_and_IDF)\n",
    "#2.4# TF-IDF\n",
    "tf_idf = tf.multiply(df_and_IDF['idf'], axis = 0)\n",
    "#print(tf_idf)\n",
    "#doc_length\n",
    "import numpy as np\n",
    "doc_length = pd.DataFrame()\n",
    "def document_length(col):\n",
    "    return np.sqrt(tf_idf[col].apply(lambda x: x**2).sum())\n",
    "for column in tf_idf.columns:\n",
    "    doc_length.loc[0, column + '_len'] = document_length(column)\n",
    "#print(doc_length)\n",
    "#NORMALIZATION TF-IDF divided by DOC_LENGTH\n",
    "normalize = pd.DataFrame()\n",
    "def get_normalize(col, x):\n",
    "    try:\n",
    "        return x / doc_length[column + '_len'].values[0]\n",
    "    except:\n",
    "        return 0\n",
    "for column in tf_idf.columns:\n",
    "    normalize[column] = tf_idf[column].apply(lambda x: get_normalize(column, x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'antony brutus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def wtf(x):\n",
    "    if x > 0:\n",
    "        return 1 + math.log(x)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Details\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tf</th>\n",
       "      <th>w_tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tf-idf</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antoni</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutu</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.522879</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>caeser</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cleopatra</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>merci</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worser</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calpurnia</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angel</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fool</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rush</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tread</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>where</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tf  w_tf       idf    tf-idf  normalized\n",
       "antoni      1     1  0.522879  0.522879    0.707107\n",
       "brutu       1     1  0.522879  0.522879    0.707107\n",
       "caeser      0     0       0.0       0.0    0.000000\n",
       "cleopatra   0     0       0.0       0.0    0.000000\n",
       "merci       0     0       0.0       0.0    0.000000\n",
       "worser      0     0       0.0       0.0    0.000000\n",
       "calpurnia   0     0       0.0       0.0    0.000000\n",
       "angel       0     0       0.0       0.0    0.000000\n",
       "fool        0     0       0.0       0.0    0.000000\n",
       "fear        0     0       0.0       0.0    0.000000\n",
       "in          0     0       0.0       0.0    0.000000\n",
       "rush        0     0       0.0       0.0    0.000000\n",
       "to          0     0       0.0       0.0    0.000000\n",
       "tread       0     0       0.0       0.0    0.000000\n",
       "where       0     0       0.0       0.0    0.000000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from numpy import count_nonzero\n",
    "import math\n",
    "query = pd.DataFrame(index = normalize.index)\n",
    "x = []\n",
    "for word in q.split():\n",
    "    x.append(ps.stem(word))\n",
    "#tf of query\n",
    "query['tf'] = [x.count(term) if term in x else 0 for term in list(normalize.index)]\n",
    "query['w_tf'] = query['tf'].apply(lambda x : int(wtf(x)))\n",
    "query['idf'] = df_and_IDF['idf'] * query['w_tf']\n",
    "query['tf-idf'] = query['tf'] * query['idf']\n",
    "query['normalized'] = 0\n",
    "product = normalize.multiply(query['w_tf'], axis = 0)\n",
    "for i in range(len(query)):\n",
    "    #normalizarion of each term in query\n",
    "    query['normalized'].iloc[i] = float(query['idf'].iloc[i]) / math.sqrt(sum(query['idf'].values**2))\n",
    "#normailze of each term in document * normalize of each term in query\n",
    "product2 = product.multiply(query['normalized'], axis = 0)\n",
    "\n",
    "print(\"Query Details\")\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "#get product2 (normalize of document1*normalize of query, ormalize of document2*normalize of query)\n",
    "for col in product2:\n",
    "#if 0 is in any columns it is removed because no matter in sum 0\n",
    "    if 0 in product2[col].loc[x].values:\n",
    "        pass\n",
    "    else:\n",
    "        #sum of column1 that was normalize of doc1 * normalize of query\n",
    "        scores[col] = product2[col].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Length\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7394622130520805"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#query length\n",
    "print(\"Query Length\")\n",
    "math.sqrt(sum([i**2 for i in query['idf'].loc[x]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product (query*matched doc)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>antoni</th>\n",
       "      <td>0.269196</td>\n",
       "      <td>0.288939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brutu</th>\n",
       "      <td>0.269196</td>\n",
       "      <td>0.288939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            doc1      doc2\n",
       "antoni  0.269196  0.288939\n",
       "brutu   0.269196  0.288939"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result of product normalize of document * normalize of query\n",
    "result = product2[list(scores.keys())].loc[x]\n",
    "print('Product (query*matched doc)')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc1    0.538393\n",
       "doc2    0.577877\n",
       "dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('doc2', 0.5778771030041435), ('doc1', 0.5383927937463102)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_scored = sorted(scores.items(), key = lambda x :x[1], reverse=True)\n",
    "final_scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc2 doc1 "
     ]
    }
   ],
   "source": [
    "for doc in final_scored:\n",
    "    print(doc[0], end =' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
